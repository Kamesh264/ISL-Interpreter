{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'core' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n",
      "File \u001b[1;32mc:\\Users\\Kamesh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mediapipe\\__init__.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msolutions\u001b[39;00m \n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtasks\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m framework\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m gpu\n",
      "File \u001b[1;32mc:\\Users\\Kamesh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mediapipe\\tasks\\python\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The MediaPipe Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"MediaPipe Tasks API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m components\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n",
      "File \u001b[1;32mc:\\Users\\Kamesh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mediapipe\\tasks\\python\\audio\\__init__.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m AudioEmbedderOptions \u001b[38;5;241m=\u001b[39m audio_embedder\u001b[38;5;241m.\u001b[39mAudioEmbedderOptions\n\u001b[0;32m     26\u001b[0m AudioEmbedderResult \u001b[38;5;241m=\u001b[39m audio_embedder\u001b[38;5;241m.\u001b[39mAudioEmbedderResult\n\u001b[1;32m---> 27\u001b[0m RunningMode \u001b[38;5;241m=\u001b[39m \u001b[43mcore\u001b[49m\u001b[38;5;241m.\u001b[39maudio_task_running_mode\u001b[38;5;241m.\u001b[39mAudioTaskRunningMode\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Remove unnecessary modules to avoid duplication in API docs.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m audio_classifier\n",
      "\u001b[1;31mNameError\u001b[0m: name 'core' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import pickle\n",
    "import mediapipe as mp\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kamesh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.4.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Kamesh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.4.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Kamesh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.4.0 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open('trained_models/scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "with open('trained_models/random_forest_model.pkl', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mp_hands \u001b[38;5;241m=\u001b[39m \u001b[43mmp\u001b[49m\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mhands\n\u001b[0;32m      2\u001b[0m mp_drawing \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mdrawing_utils\n\u001b[0;32m      4\u001b[0m confidence \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mp' is not defined"
     ]
    }
   ],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "confidence = 0.5\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how,you : How are you?\n",
      "how,you,name : What is you name?\n",
      "where,you,house : Where do you live?\n",
      "nice,meet,you : Nice to meet you.\n",
      "me,sorry : I am sorry.\n",
      "me,love,learn : I love learning\n",
      "you,look,nice : You look beautliful.\n",
      "me,love,you : I love you.\n",
      "me,you,meet : Shall we meet?\n",
      "help,me : Please help me.\n",
      "where,you : Where are you?\n",
      "me,look,help : I am looking for help\n",
      "how,help,me,you : How can I help you?\n",
      "learn,you,me : Let's learn together\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"landmark_data/Gestures_sentences.csv\")\n",
    "my_dict = df.set_index('gesture_names')['sentence'].to_dict()\n",
    "for key in my_dict:\n",
    "    print(key, ':', my_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how you : How are you?\n",
      "how you name : What is you name?\n",
      "where you house : Where do you live?\n",
      "nice meet you : Nice to meet you.\n",
      "me sorry : I am sorry.\n",
      "me love learn : I love learning\n",
      "you look nice : You look beautliful.\n",
      "me love you : I love you.\n",
      "me you meet : Shall we meet?\n",
      "help me : Please help me.\n",
      "where you : Where are you?\n",
      "me look help : I am looking for help\n",
      "how help me you : How can I help you?\n",
      "learn you me : Let's learn together\n"
     ]
    }
   ],
   "source": [
    "final_dict = {}\n",
    "for key in my_dict:\n",
    "    t = []\n",
    "    words = key.split(',')\n",
    "    for word in words:\n",
    "        t.append(word)\n",
    "    s = ' '.join(t)\n",
    "    final_dict[s] = my_dict[key]\n",
    "for key in final_dict:\n",
    "    print(key, ':', final_dict[key])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_limit = 3 \n",
    "def generate_caption(word, seq):\n",
    "    res = ''\n",
    "    if len(seq) < word_limit:\n",
    "        seq.append(word)\n",
    "        seq.append(word)\n",
    "        s = ' '.join(seq)\n",
    "        if s in final_dict:\n",
    "            res = final_dict[s]\n",
    "\n",
    "    elif len(seq) == word_limit:\n",
    "        seq.pop(0)\n",
    "        s = ' '.join(seq)\n",
    "        if s in final_dict:\n",
    "            res = final_dict[s]\n",
    "        seq.append(word)\n",
    "        s = ' '.join(seq)\n",
    "        if s in final_dict:\n",
    "            res = final_dict[s]   \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "threshold_list = []\n",
    "threshold = 20\n",
    "seq = ['None']\n",
    "caption = ''\n",
    "prev_caption = ''\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, image = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        continue\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "\n",
    "    results = hands.process(image)\n",
    "\n",
    "    image.flags.writeable = True\n",
    "    \n",
    "    #converting the BGR image to RGB.\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    both_hand_landmarks = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            landmarks = []\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                # Extract x, y coordinates (relative to image dimensions)\n",
    "                x = landmark.x\n",
    "                y = landmark.y\n",
    "                # Append coordinates to the list\n",
    "                landmarks.append((x, y))\n",
    "            both_hand_landmarks.append(landmarks)\n",
    "        \n",
    "        if len(both_hand_landmarks) == 1:\n",
    "            both_hand_landmarks.append([(0, 0)] * len(both_hand_landmarks[0]))\n",
    "        values = list(np.array(both_hand_landmarks).flatten())\n",
    "        values = scaler.transform([values])\n",
    "        predicted = loaded_model.predict(values)\n",
    "\n",
    "        cv2.rectangle(image, (0,0), (160, 60), (245, 90, 16), -1)\n",
    "        # Displaying Class\n",
    "        cv2.putText(image, 'Predicted Gesture'\n",
    "                    , (20,15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(predicted[0])\n",
    "                    , (20,45), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)  \n",
    "        threshold_list.append(predicted[0])\n",
    "\n",
    "        if threshold_list.count(predicted[0]) >= threshold:\n",
    "            # Add caption text\n",
    "            if seq[-1] != predicted[0]:\n",
    "                caption = generate_caption(predicted[0], seq)\n",
    "            if caption == '':\n",
    "                caption= prev_caption\n",
    "            else:\n",
    "                prev_caption = caption\n",
    "            threshold_list = []\n",
    "    caption_size = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)[0]\n",
    "    caption_x = int((image.shape[1] - caption_size[0]) / 2)\n",
    "    caption_y = image.shape[0] - 10  # Adjust 10 for padding\n",
    "    cv2.putText(image, caption, (caption_x, caption_y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    \n",
    "    cv2.imshow('Sign Translator', image)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
